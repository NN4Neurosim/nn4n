## Verson 1.1.0 Change Logs
### Changes for `CTRNN`
1. Merged `input_dim`, `hidden_size`, and `output_dim` into `dims`.
2. Removed `scaling`. This is now split into `weights` by manually scaling the weight matrices.
3. Removed `layer_masks`. This is now split into `plasticity_masks`, `ei_masks`, and `sparsity_masks` to increase flexibility and reduce confusion.
4. Renamed `sparsity_constraints` to `sparsity_masks`. `sparsity_masks` inherits everything from `sparsity_constraints`, but can also be masks that can be used to control the sparsity of every single weight in the network.
5. Renamed `positivity_constraints` to `ei_masks`. `ei_masks` inherits everything from `positivity_constraints`, but can also be masks that can be used to control the positivity of every single weight in the network.
6. Renamed `layer_biases` to `biases`. `biases` inherits everything from `layer_biases`, but can also be a list of bias vectors that used to directly initialize the biases of the network.
7. Renamed and upgraded `layer_distributions` to `weights`. `weights` inherits everything from `layer_distributions`, but can also be a list of weight matrices that used to directly initialize the weights of the network.
8. Rename and upgraded `learnable` to `plasticity_masks`. `plasticity_masks` inherits everything from `learnable`, but can also be masks that can be used to control the plasticity of every single weight in the network.
9. Add `auto_rescale()` to `CTRNN`. If you set a sparsity mask, calling this function will help you automatically rescale the weights to make sure the sparse matrix can still drive the network dynamics. (Consider if you make sparsity = 0.01, the output is unlikely to be able to drive the next layer).

### Changes for `structure`
1. The structure module won't be maintained in future versions. Everything is now inherited by `mask`.

### Changes for `mask`
1. Inherited everything from `structure`.
2. The `mask` module now recieve a `dims` parameter instead of `input_dim`, `hidden_size`, and `output_dim`.

## TODOs
- [x] Put initialization to structures, i.e., the distribution of weights, biases, etc.
- [x] Add custom controls to the update speeds of different parts of the network.
- [x] Remove `scaling` from `CTRNN`. (duplicated)
- [ ] ~~Move `self_connections` to `structure` class.~~
- [ ] ~~Remove `layer_biases`.~~ (duplicated)
- [ ] Fix documentation for `print_layers`, add `plot_layers`.
- [ ] ~~Let `structure` be a parameter of `CTRNN`.~~
- [x] Rename and upgraded `learnable` to `plasticity_masks`. `plasticity_masks` inherits everything from `learnable`, but can also be masks that can be used to control the plasticity of every single weight in the network.
- [x] Rename and upgraded `layer_distributions` to `weights`. `weights` inherits everything from `layer_distributions`, but can also be a list of weight matrices that used to directly initialize the weights of the network.
- [x] Rename `layer_biases` to `biases`. `biases` inherits everything from `layer_biases`, but can also be a list of bias vectors that used to directly initialize the biases of the network.
- [x] Rename `positivity_constraints` to `ei_masks`. `ei_masks` inherits everything from `positivity_constraints`, but can also be masks that can be used to control the positivity of every single weight in the network.
- [x] Rename `sparsity_constraints` to `sparsity_masks`. `sparsity_masks` inherits everything from `sparsity_constraints`, but can also be masks that can be used to control the sparsity of every single weight in the network.
- [x] Remove `layer_masks`. This is now split into `plasticity_masks`, `ei_masks`, and `sparsity_masks` to increase flexibility and reduce confusion.
- [x] Remove `scaling`. This is now split into `weights` by manually scaling the weight matrices.
- [x] Merge `input_dim`, `hidden_size`, and `output_dim` into `dims`.
- [ ] Rewrite the dimension check for masks. Currently the masks are not intuitive. One way is to transpose everything in `mask` module, and transpose them back when received by `CTRNN`. Don't add auto-compatible feature for now as it might mess up the hidden layer since its a sqare matrix.
- [ ] Add deprecation autofix.
- [ ] Test `init_state` (how hidden states are initialized).
- [x] Add `auto_rescale()` to `CTRNN`.
- [ ] Check _balance_excitatory_inhibitory().
- [ ] Put `self_connections` into `sparsity_masks`.
- [x] Rename `structure` module to `mask`.
- [x] Add function `apply_plasticity()` to accommodate the new `plasticity_masks`.
- [ ] ~~Add warning for unnecessary call of `apply_plasticity()` when all plasticity masks are 1.~~ (no need as it might be good to check gradients anyway)