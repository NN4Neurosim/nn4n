## Verson 1.1.0 Change Logs
1. Merged `input_dim`, `hidden_size`, and `output_dim` into `dims`.
2. Removed `scaling`. This is now split into `weights` by manually scaling the weight matrices.
3. Removed `layer_masks`. This is now split into `plasticity_masks`, `ei_masks`, and `sparsity_masks` to increase flexibility and reduce confusion.
4. Renamed `sparsity_constraints` to `sparsity_masks`. `sparsity_masks` inherits everything from `sparsity_constraints`, but can also be masks that can be used to control the sparsity of every single weight in the network.
5. Renamed `positivity_constraints` to `ei_masks`. `ei_masks` inherits everything from `positivity_constraints`, but can also be masks that can be used to control the positivity of every single weight in the network.
6. Renamed `layer_biases` to `biases`. `biases` inherits everything from `layer_biases`, but can also be a list of bias vectors that used to directly initialize the biases of the network.
7. Renamed and upgraded `layer_distributions` to `weights`. `weights` inherits everything from `layer_distributions`, but can also be a list of weight matrices that used to directly initialize the weights of the network.
8. Rename and upgraded `learnable` to `plasticity_masks`. `plasticity_masks` inherits everything from `learnable`, but can also be masks that can be used to control the plasticity of every single weight in the network.
9. Add `auto_rescale()` to `CTRNN`. If you set a sparsity mask, calling this function will help you automatically rescale the weights to make sure the sparse matrix can still drive the network dynamics. (Consider if you make sparsity = 0.01, the output is unlikely to be able to drive the next layer).

## TODOs
- [x] Put initialization to structures, i.e., the distribution of weights, biases, etc.
- [x] Add custom controls to the update speeds of different parts of the network.
- [x] Remove `scaling` from `CTRNN`. (duplicated)
- [ ] ~~Move `self_connections` to `structure` class.~~
- [ ] ~~Remove `layer_biases`.~~ (duplicated)
- [ ] Fix documentation for `print_layers`, add `plot_layers`.
- [ ] ~~Let `structure` be a parameter of `CTRNN`.~~
- [x] Rename and upgraded `learnable` to `plasticity_masks`. `plasticity_masks` inherits everything from `learnable`, but can also be masks that can be used to control the plasticity of every single weight in the network.
- [x] Rename and upgraded `layer_distributions` to `weights`. `weights` inherits everything from `layer_distributions`, but can also be a list of weight matrices that used to directly initialize the weights of the network.
- [x] Rename `layer_biases` to `biases`. `biases` inherits everything from `layer_biases`, but can also be a list of bias vectors that used to directly initialize the biases of the network.
- [x] Rename `positivity_constraints` to `ei_masks`. `ei_masks` inherits everything from `positivity_constraints`, but can also be masks that can be used to control the positivity of every single weight in the network.
- [x] Rename `sparsity_constraints` to `sparsity_masks`. `sparsity_masks` inherits everything from `sparsity_constraints`, but can also be masks that can be used to control the sparsity of every single weight in the network.
- [x] Remove `layer_masks`. This is now split into `plasticity_masks`, `ei_masks`, and `sparsity_masks` to increase flexibility and reduce confusion.
- [x] Remove `scaling`. This is now split into `weights` by manually scaling the weight matrices.
- [x] Merge `input_dim`, `hidden_size`, and `output_dim` into `dims`.
- [ ] Check the checker function dimension check feature.
- [ ] Add deprecation auto-fixing.
- [ ] Test `init_state`.
- [x] Add `auto_rescale()` to `CTRNN`.
- [ ] Check _balance_excitatory_inhibitory().
- [ ] Put `self_connections` into `sparsity_masks`.